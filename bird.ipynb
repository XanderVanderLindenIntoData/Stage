{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Setup MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_mysql():\n",
    "    # Open database connection\n",
    "    # Connect to the database\"\n",
    "    db = pymysql.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"root\",\n",
    "        database=\"BIRD\",\n",
    "        # port=3306,\n",
    "    )\n",
    "    return db\n",
    "\n",
    "\n",
    "def execute_mysql_query(cursor, query):\n",
    "    \"\"\"Execute a MySQL query.\"\"\"\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    return result\n",
    "\n",
    "\n",
    "def perform_query_on_mysql_databases(query):\n",
    "    db = connect_mysql()\n",
    "    cursor = db.cursor()\n",
    "    result = execute_mysql_query(cursor, query)\n",
    "    db.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_query = \"\"\"\n",
    "SELECT  CAST(SUM(CASE WHEN `Currency` = 'EUR' THEN 1 ELSE 0 END) AS DOUBLE) / SUM(CASE WHEN `Currency` = 'CZK' THEN 1 ELSE 0 END) FROM `customers`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_result = perform_query_on_mysql_databases(mysql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.06572769953051644,),)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mysql_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIRD PostGRESQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import sqlalchemy\n",
    "\n",
    "def connect_postgresql():\n",
    "    # Open database connection\n",
    "    # Connect to the database\n",
    "    db = psycopg2.connect(\n",
    "        \"dbname=BIRD user=postgres host=localhost password=root port=5432\"\n",
    "    )\n",
    "    return db\n",
    "\n",
    "\n",
    "def execute_postgresql_query(cursor, query):\n",
    "    \"\"\"Execute a MySQL query.\"\"\"\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    return result\n",
    "\n",
    "\n",
    "def perform_query_on_postgresql_databases(query):\n",
    "    db = connect_postgresql()\n",
    "    cursor = db.cursor()\n",
    "    result = execute_postgresql_query(cursor, query)\n",
    "    db.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresql_query = \"SELECT CAST(SUM(CASE WHEN Currency = 'EUR' THEN 1 ELSE 0 END) AS REAL) / NULLIF(SUM(CASE WHEN Currency = 'CZK' THEN 1 ELSE 0 END), 0) FROM customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresql_result = perform_query_on_postgresql_databases(postgresql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.06572769953051644,)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postgresql_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Files Found:\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\california_schools\\database_description\\frpm.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\california_schools\\database_description\\satscores.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\california_schools\\database_description\\schools.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\card_games\\database_description\\cards.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\card_games\\database_description\\foreign_data.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\card_games\\database_description\\legalities.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\card_games\\database_description\\rulings.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\card_games\\database_description\\sets.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\card_games\\database_description\\set_translations.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\codebase_community\\database_description\\badges.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\codebase_community\\database_description\\comments.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\codebase_community\\database_description\\postHistory.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\codebase_community\\database_description\\postLinks.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\codebase_community\\database_description\\posts.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\codebase_community\\database_description\\tags.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\codebase_community\\database_description\\users.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\codebase_community\\database_description\\votes.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\debit_card_specializing\\database_description\\customers.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\debit_card_specializing\\database_description\\gasstations.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\debit_card_specializing\\database_description\\products.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\debit_card_specializing\\database_description\\transactions_1k.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\debit_card_specializing\\database_description\\yearmonth.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\european_football_2\\database_description\\Country.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\european_football_2\\database_description\\League.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\european_football_2\\database_description\\Match.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\european_football_2\\database_description\\Player.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\european_football_2\\database_description\\Player_Attributes.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\european_football_2\\database_description\\Team.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\european_football_2\\database_description\\Team_Attributes.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\financial\\database_description\\account.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\financial\\database_description\\card.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\financial\\database_description\\client.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\financial\\database_description\\disp.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\financial\\database_description\\district.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\financial\\database_description\\loan.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\financial\\database_description\\order.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\financial\\database_description\\trans.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\circuits.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\constructorResults.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\constructors.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\constructorStandings.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\drivers.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\driverStandings.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\lapTimes.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\pitStops.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\qualifying.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\races.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\results.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\seasons.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\formula_1\\database_description\\status.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\student_club\\database_description\\Attendance.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\student_club\\database_description\\Budget.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\student_club\\database_description\\Event.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\student_club\\database_description\\Expense.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\student_club\\database_description\\Income.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\student_club\\database_description\\Major.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\student_club\\database_description\\Member.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\student_club\\database_description\\Zip_Code.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\superhero\\database_description\\alignment.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\superhero\\database_description\\attribute.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\superhero\\database_description\\colour.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\superhero\\database_description\\gender.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\superhero\\database_description\\hero_attribute.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\superhero\\database_description\\hero_power.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\superhero\\database_description\\publisher.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\superhero\\database_description\\race.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\superhero\\database_description\\superhero.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\superhero\\database_description\\superpower.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\thrombosis_prediction\\database_description\\Examination.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\thrombosis_prediction\\database_description\\Laboratory.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\thrombosis_prediction\\database_description\\Patient.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\toxicology\\database_description\\atom.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\toxicology\\database_description\\bond.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\toxicology\\database_description\\connected.csv\n",
      "C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\\dev_databases\\toxicology\\database_description\\molecule.csv\n",
      "\n",
      "Loaded mini_dev_postgresql.json successfully.\n",
      "Loaded dev_tables.json successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Base MINIDEV directory\n",
    "MINIDEV_PATH = r\"C:\\Users\\Xander\\IntoData\\IntoDataStage\\Stage\\minidev\\minidev\\MINIDEV\"\n",
    "\n",
    "# 1. Find all CSVs in dev_databases/*/database_description/\n",
    "csv_files = glob.glob(os.path.join(MINIDEV_PATH, \"dev_databases\", \"*\", \"database_description\", \"*.csv\"))\n",
    "\n",
    "print(\"CSV Files Found:\")\n",
    "for csv in csv_files:\n",
    "    print(csv)\n",
    "\n",
    "# 2. Locate JSON files\n",
    "mini_dev_json_path = os.path.join(MINIDEV_PATH, \"mini_dev_postgresql.json\")\n",
    "dev_tables_json_path = os.path.join(MINIDEV_PATH, \"dev_tables.json\")\n",
    "\n",
    "# 3. Load JSON files into Pandas DataFrames\n",
    "if os.path.exists(mini_dev_json_path):\n",
    "    mini_dev_df = pd.read_json(mini_dev_json_path)\n",
    "    print(\"\\nLoaded mini_dev_postgresql.json successfully.\")\n",
    "else:\n",
    "    print(\"\\nError: mini_dev_postgresql.json not found!\")\n",
    "\n",
    "if os.path.exists(dev_tables_json_path):\n",
    "    table_names_df = pd.read_json(dev_tables_json_path)\n",
    "    print(\"Loaded dev_tables.json successfully.\")\n",
    "else:\n",
    "    print(\"Error: dev_tables.json not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>db_id</th>\n",
       "      <th>question</th>\n",
       "      <th>evidence</th>\n",
       "      <th>SQL</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1471</td>\n",
       "      <td>debit_card_specializing</td>\n",
       "      <td>What is the ratio of customers who pay in EUR ...</td>\n",
       "      <td>ratio of customers who pay in EUR against cust...</td>\n",
       "      <td>SELECT CAST(SUM(CASE WHEN Currency = 'EUR' THE...</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1472</td>\n",
       "      <td>debit_card_specializing</td>\n",
       "      <td>In 2012, who had the least consumption in LAM?</td>\n",
       "      <td>Year 2012 can be presented as Between 201201 A...</td>\n",
       "      <td>SELECT T1.CustomerID FROM customers AS T1 INNE...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1473</td>\n",
       "      <td>debit_card_specializing</td>\n",
       "      <td>What was the average monthly consumption of cu...</td>\n",
       "      <td>Average Monthly consumption = AVG(Consumption)...</td>\n",
       "      <td>SELECT AVG(T2.Consumption) / NULLIF(12, 0) FRO...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476</td>\n",
       "      <td>debit_card_specializing</td>\n",
       "      <td>What was the difference in gas consumption bet...</td>\n",
       "      <td>cast the consumption into float when perform c...</td>\n",
       "      <td>SELECT SUM(CASE WHEN T1.Currency = 'CZK' THEN ...</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1479</td>\n",
       "      <td>debit_card_specializing</td>\n",
       "      <td>Which year recorded the most consumption of ga...</td>\n",
       "      <td>The first 4 strings of the Date values in the ...</td>\n",
       "      <td>SELECT SUBSTR(T2.Date, 1, 4) FROM customers AS...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                    db_id  \\\n",
       "0         1471  debit_card_specializing   \n",
       "1         1472  debit_card_specializing   \n",
       "2         1473  debit_card_specializing   \n",
       "3         1476  debit_card_specializing   \n",
       "4         1479  debit_card_specializing   \n",
       "\n",
       "                                            question  \\\n",
       "0  What is the ratio of customers who pay in EUR ...   \n",
       "1     In 2012, who had the least consumption in LAM?   \n",
       "2  What was the average monthly consumption of cu...   \n",
       "3  What was the difference in gas consumption bet...   \n",
       "4  Which year recorded the most consumption of ga...   \n",
       "\n",
       "                                            evidence  \\\n",
       "0  ratio of customers who pay in EUR against cust...   \n",
       "1  Year 2012 can be presented as Between 201201 A...   \n",
       "2  Average Monthly consumption = AVG(Consumption)...   \n",
       "3  cast the consumption into float when perform c...   \n",
       "4  The first 4 strings of the Date values in the ...   \n",
       "\n",
       "                                                 SQL   difficulty  \n",
       "0  SELECT CAST(SUM(CASE WHEN Currency = 'EUR' THE...       simple  \n",
       "1  SELECT T1.CustomerID FROM customers AS T1 INNE...     moderate  \n",
       "2  SELECT AVG(T2.Consumption) / NULLIF(12, 0) FRO...     moderate  \n",
       "3  SELECT SUM(CASE WHEN T1.Currency = 'CZK' THEN ...  challenging  \n",
       "4  SELECT SUBSTR(T2.Date, 1, 4) FROM customers AS...     moderate  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema linking\n",
    "\n",
    "Belangrijke kolomnamen uit input nlp halen\n",
    "Ik ga dit doen met in Context-learning op basis van een pipeline met 3 stappen. De entity en context retrieval schema selection en query generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from bridge_content_encoder import get_database_matches\n",
    "from sql_metadata import Parser\n",
    "from tqdm import tqdm\n",
    "\n",
    "sql_keywords = ['select', 'from', 'where', 'group', 'order', 'limit', 'intersect', 'union', \\\n",
    "                'except', 'join', 'on', 'as', 'not', 'between', 'in', 'like', 'is', 'exists', 'max', 'min', \\\n",
    "                'count', 'sum', 'avg', 'and', 'or', 'desc', 'asc']\n",
    "\n",
    "\n",
    "def parse_option():\n",
    "    parser = argparse.ArgumentParser(\"\")\n",
    "\n",
    "    parser.add_argument('--mode', type=str, default=\"train\")\n",
    "    parser.add_argument('--table_path', type=str, default=\"./data/spider/tables.json\")\n",
    "    parser.add_argument('--input_dataset_path', type=str, default=\"./data/spider/train_spider.json\",\n",
    "                        help='''\n",
    "                            options:\n",
    "                                ./data/spider/train_spider.json\n",
    "                                ./data/spider/dev.json\n",
    "                            ''')\n",
    "    parser.add_argument('--natsql_dataset_path', type=str, default=\"./NatSQL/NatSQLv1_6/train_spider-natsql.json\",\n",
    "                        help='''\n",
    "                            options:\n",
    "                                ./NatSQL/NatSQLv1_6/train_spider-natsql.json\n",
    "                                ./NatSQL/NatSQLv1_6/dev-natsql.json\n",
    "                            ''')\n",
    "    parser.add_argument('--output_dataset_path', type=str, default=\"./data/pre-processing/preprocessed_dataset.json\",\n",
    "                        help=\"the filepath of preprocessed dataset.\")\n",
    "    parser.add_argument('--db_path', type=str, default=\"./data/spider/database\",\n",
    "                        help=\"the filepath of database.\")\n",
    "    parser.add_argument(\"--target_type\", type=str, default=\"sql\",\n",
    "                        help=\"sql or natsql.\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"spider\")\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    return opt\n",
    "\n",
    "\n",
    "def get_db_contents(question, table_name_original, column_names_original, db_id, db_path):\n",
    "    matched_contents = []\n",
    "    # extract matched contents for each column\n",
    "    for column_name_original in column_names_original:\n",
    "        matches = get_database_matches(\n",
    "            question,\n",
    "            table_name_original,\n",
    "            column_name_original,\n",
    "            db_path + \"/{}/{}.sqlite\".format(db_id, db_id)\n",
    "        )\n",
    "        matches = sorted(matches)\n",
    "        matched_contents.append(matches)\n",
    "\n",
    "    return matched_contents\n",
    "\n",
    "\n",
    "def get_db_schemas(all_db_infos, opt=None):\n",
    "    db_schemas = {}\n",
    "\n",
    "    for db in all_db_infos:\n",
    "        table_names_original = db[\"table_names_original\"]\n",
    "        table_names = db[\"table_names\"]\n",
    "        column_names_original = db[\"column_names_original\"]\n",
    "        column_names = db[\"column_names\"]\n",
    "        column_types = db[\"column_types\"]\n",
    "\n",
    "        db_schemas[db[\"db_id\"]] = {}\n",
    "\n",
    "        primary_keys, foreign_keys = [], []\n",
    "        # record primary keys\n",
    "        for pk_column_idx in db[\"primary_keys\"]:\n",
    "            pk_table_name_original = table_names_original[column_names_original[pk_column_idx][0]]\n",
    "            pk_column_name_original = column_names_original[pk_column_idx][1]\n",
    "\n",
    "            primary_keys.append(\n",
    "                {\n",
    "                    \"table_name_original\": pk_table_name_original.lower(),\n",
    "                    \"column_name_original\": pk_column_name_original.lower()\n",
    "                }\n",
    "            )\n",
    "\n",
    "        db_schemas[db[\"db_id\"]][\"pk\"] = primary_keys\n",
    "\n",
    "        # record foreign keys\n",
    "        for source_column_idx, target_column_idx in db[\"foreign_keys\"]:\n",
    "            fk_source_table_name_original = table_names_original[column_names_original[source_column_idx][0]]\n",
    "            fk_source_column_name_original = column_names_original[source_column_idx][1]\n",
    "\n",
    "            fk_target_table_name_original = table_names_original[column_names_original[target_column_idx][0]]\n",
    "            fk_target_column_name_original = column_names_original[target_column_idx][1]\n",
    "\n",
    "            foreign_keys.append(\n",
    "                {\n",
    "                    \"source_table_name_original\": fk_source_table_name_original.lower(),\n",
    "                    \"source_column_name_original\": fk_source_column_name_original.lower(),\n",
    "                    \"target_table_name_original\": fk_target_table_name_original.lower(),\n",
    "                    \"target_column_name_original\": fk_target_column_name_original.lower(),\n",
    "                }\n",
    "            )\n",
    "        db_schemas[db[\"db_id\"]][\"fk\"] = foreign_keys\n",
    "\n",
    "        db_schemas[db[\"db_id\"]][\"schema_items\"] = []\n",
    "        for idx, table_name_original in enumerate(table_names_original):\n",
    "            column_names_original_list = []\n",
    "            column_names_list = []\n",
    "            column_types_list = []\n",
    "            for column_idx, (table_idx, column_name_original) in enumerate(column_names_original):\n",
    "                if idx == table_idx:\n",
    "                    column_names_original_list.append(column_name_original.lower())\n",
    "                    column_names_list.append(column_names[column_idx][1].lower())\n",
    "                    column_types_list.append(column_types[column_idx])\n",
    "\n",
    "            db_schemas[db[\"db_id\"]][\"schema_items\"].append({\n",
    "                \"table_name_original\": table_name_original.lower(),\n",
    "                \"table_name\": table_names[idx].lower(),\n",
    "                \"column_names\": column_names_list,\n",
    "                \"column_names_original\": column_names_original_list,\n",
    "                \"column_types\": column_types_list\n",
    "            })\n",
    "\n",
    "    return db_schemas\n",
    "\n",
    "\n",
    "def normalization(sql):\n",
    "    def white_space_fix(s):\n",
    "        parsed_s = Parser(s)\n",
    "        s = \" \".join([token.value for token in parsed_s.tokens])\n",
    "\n",
    "        return s\n",
    "\n",
    "    # convert everything except text between single quotation marks to lower case\n",
    "    def lower(s):\n",
    "        in_quotation = False\n",
    "        out_s = \"\"\n",
    "        for char in s:\n",
    "            if in_quotation:\n",
    "                out_s += char\n",
    "            else:\n",
    "                out_s += char.lower()\n",
    "\n",
    "            if char == \"'\":\n",
    "                if in_quotation:\n",
    "                    in_quotation = False\n",
    "                else:\n",
    "                    in_quotation = True\n",
    "\n",
    "        return out_s\n",
    "\n",
    "    # remove \";\"\n",
    "    def remove_semicolon(s):\n",
    "        if s.endswith(\";\"):\n",
    "            s = s[:-1]\n",
    "        return s\n",
    "\n",
    "    # double quotation -> single quotation \n",
    "    def double2single(s):\n",
    "        return s.replace(\"\\\"\", \"'\")\n",
    "\n",
    "    def add_asc(s):\n",
    "        pattern = re.compile(\n",
    "            r'order by (?:\\w+ \\( \\S+ \\)|\\w+\\.\\w+|\\w+)(?: (?:\\+|\\-|\\<|\\<\\=|\\>|\\>\\=) (?:\\w+ \\( \\S+ \\)|\\w+\\.\\w+|\\w+))*')\n",
    "        if \"order by\" in s and \"asc\" not in s and \"desc\" not in s:\n",
    "            for p_str in pattern.findall(s):\n",
    "                s = s.replace(p_str, p_str + \" asc\")\n",
    "\n",
    "        return s\n",
    "\n",
    "    def remove_table_alias(s):\n",
    "        tables_aliases = Parser(s).tables_aliases\n",
    "        new_tables_aliases = {}\n",
    "        for i in range(1, 11):\n",
    "            if \"t{}\".format(i) in tables_aliases.keys():\n",
    "                new_tables_aliases[\"t{}\".format(i)] = tables_aliases[\"t{}\".format(i)]\n",
    "\n",
    "        tables_aliases = new_tables_aliases\n",
    "        for k, v in tables_aliases.items():\n",
    "            s = s.replace(\"as \" + k + \" \", \"\")\n",
    "            s = s.replace(k, v)\n",
    "\n",
    "        return s\n",
    "\n",
    "    processing_func = lambda x: remove_table_alias(add_asc(lower(white_space_fix(double2single(remove_semicolon(x))))))\n",
    "\n",
    "    return processing_func(sql)\n",
    "\n",
    "\n",
    "# extract the skeleton of sql and natsql\n",
    "def extract_skeleton(sql, db_schema):\n",
    "    table_names_original, table_dot_column_names_original, column_names_original = [], [], []\n",
    "    for table in db_schema[\"schema_items\"]:\n",
    "        table_name_original = table[\"table_name_original\"]\n",
    "        table_names_original.append(table_name_original)\n",
    "\n",
    "        for column_name_original in [\"*\"] + table[\"column_names_original\"]:\n",
    "            table_dot_column_names_original.append(table_name_original + \".\" + column_name_original)\n",
    "            column_names_original.append(column_name_original)\n",
    "\n",
    "    parsed_sql = Parser(sql)\n",
    "    new_sql_tokens = []\n",
    "    for token in parsed_sql.tokens:\n",
    "        # mask table names\n",
    "        if token.value in table_names_original:\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        # mask column names\n",
    "        elif token.value in column_names_original \\\n",
    "                or token.value in table_dot_column_names_original:\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        # mask string values\n",
    "        elif token.value.startswith(\"'\") and token.value.endswith(\"'\"):\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        # mask positive int number\n",
    "        elif token.value.isdigit():\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        # mask negative int number\n",
    "        elif isNegativeInt(token.value):\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        # mask float number\n",
    "        elif isFloat(token.value):\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        else:\n",
    "            new_sql_tokens.append(token.value.strip())\n",
    "\n",
    "    sql_skeleton = \" \".join(new_sql_tokens)\n",
    "\n",
    "    # remove JOIN ON keywords\n",
    "    sql_skeleton = sql_skeleton.replace(\"on _ = _ and _ = _\", \"on _ = _\")\n",
    "    sql_skeleton = sql_skeleton.replace(\"on _ = _ or _ = _\", \"on _ = _\")\n",
    "    sql_skeleton = sql_skeleton.replace(\" on _ = _\", \"\")\n",
    "    pattern3 = re.compile(\"_ (?:join _ ?)+\")\n",
    "    sql_skeleton = re.sub(pattern3, \"_ \", sql_skeleton)\n",
    "\n",
    "    # \"_ , _ , ..., _\" -> \"_\"\n",
    "    while (\"_ , _\" in sql_skeleton):\n",
    "        sql_skeleton = sql_skeleton.replace(\"_ , _\", \"_\")\n",
    "\n",
    "    # remove clauses in WHERE keywords\n",
    "    ops = [\"=\", \"!=\", \">\", \">=\", \"<\", \"<=\"]\n",
    "    for op in ops:\n",
    "        if \"_ {} _\".format(op) in sql_skeleton:\n",
    "            sql_skeleton = sql_skeleton.replace(\"_ {} _\".format(op), \"_\")\n",
    "    while (\"where _ and _\" in sql_skeleton or \"where _ or _\" in sql_skeleton):\n",
    "        if \"where _ and _\" in sql_skeleton:\n",
    "            sql_skeleton = sql_skeleton.replace(\"where _ and _\", \"where _\")\n",
    "        if \"where _ or _\" in sql_skeleton:\n",
    "            sql_skeleton = sql_skeleton.replace(\"where _ or _\", \"where _\")\n",
    "\n",
    "    # remove additional spaces in the skeleton\n",
    "    while \"  \" in sql_skeleton:\n",
    "        sql_skeleton = sql_skeleton.replace(\"  \", \" \")\n",
    "\n",
    "    return sql_skeleton\n",
    "\n",
    "\n",
    "def isNegativeInt(string):\n",
    "    if string.startswith(\"-\") and string[1:].isdigit():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def isFloat(string):\n",
    "    if string.startswith(\"-\"):\n",
    "        string = string[1:]\n",
    "\n",
    "    s = string.split(\".\")\n",
    "    if len(s) > 2:\n",
    "        return False\n",
    "    else:\n",
    "        for s_i in s:\n",
    "            if not s_i.isdigit():\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    dataset = json.load(open(opt.input_dataset_path))\n",
    "    all_db_infos = json.load(open(opt.table_path))\n",
    "\n",
    "    assert opt.mode in [\"train\", \"eval\", \"test\"]\n",
    "\n",
    "    if opt.mode in [\"train\", \"eval\"] and opt.target_type == \"natsql\":\n",
    "        # only train_spider.json and dev.json have corresponding natsql dataset\n",
    "        natsql_dataset = json.load(open(opt.natsql_dataset_path))\n",
    "    else:\n",
    "        # empty natsql dataset\n",
    "        natsql_dataset = [None for _ in range(len(dataset))]\n",
    "\n",
    "    db_schemas = get_db_schemas(all_db_infos, opt)\n",
    "\n",
    "    preprocessed_dataset = []\n",
    "\n",
    "    for natsql_data, data in tqdm(zip(natsql_dataset, dataset)):\n",
    "        if data[\n",
    "            'query'] == 'SELECT T1.company_name FROM Third_Party_Companies AS T1 JOIN Maintenance_Contracts AS T2 ON T1.company_id  =  T2.maintenance_contract_company_id JOIN Ref_Company_Types AS T3 ON T1.company_type_code  =  T3.company_type_code ORDER BY T2.contract_end_date DESC LIMIT 1':\n",
    "            data[\n",
    "                'query'] = 'SELECT T1.company_type FROM Third_Party_Companies AS T1 JOIN Maintenance_Contracts AS T2 ON T1.company_id  =  T2.maintenance_contract_company_id ORDER BY T2.contract_end_date DESC LIMIT 1'\n",
    "            data['query_toks'] = ['SELECT', 'T1.company_type', 'FROM', 'Third_Party_Companies', 'AS', 'T1', 'JOIN',\n",
    "                                  'Maintenance_Contracts', 'AS', 'T2', 'ON', 'T1.company_id', '=',\n",
    "                                  'T2.maintenance_contract_company_id', 'ORDER', 'BY', 'T2.contract_end_date',\n",
    "                                  'DESC',\n",
    "                                  'LIMIT', '1']\n",
    "            data['query_toks_no_value'] = ['select', 't1', '.', 'company_type', 'from', 'third_party_companies',\n",
    "                                           'as',\n",
    "                                           't1', 'join', 'maintenance_contracts', 'as', 't2', 'on', 't1', '.',\n",
    "                                           'company_id', '=', 't2', '.', 'maintenance_contract_company_id', 'order',\n",
    "                                           'by', 't2', '.', 'contract_end_date', 'desc', 'limit', 'value']\n",
    "            data['question'] = 'What is the type of the company who concluded its contracts most recently?'\n",
    "            data['question_toks'] = ['What', 'is', 'the', 'type', 'of', 'the', 'company', 'who', 'concluded', 'its',\n",
    "                                     'contracts', 'most', 'recently', '?']\n",
    "        if data['query'].startswith(\n",
    "                'SELECT T1.fname FROM student AS T1 JOIN lives_in AS T2 ON T1.stuid  =  T2.stuid WHERE T2.dormid IN'):\n",
    "            data['query'] = data['query'].replace('IN (SELECT T2.dormid)', 'IN (SELECT T3.dormid)')\n",
    "            index = data['query_toks'].index('(') + 2\n",
    "            assert data['query_toks'][index] == 'T2.dormid'\n",
    "            data['query_toks'][index] = 'T3.dormid'\n",
    "            index = data['query_toks_no_value'].index('(') + 2\n",
    "            assert data['query_toks_no_value'][index] == 't2'\n",
    "            data['query_toks_no_value'][index] = 't3'\n",
    "\n",
    "        question = data[\"question\"].replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\").replace(\"\\u201c\", \"'\").replace(\n",
    "            \"\\u201d\", \"'\").strip()\n",
    "        db_id = data[\"db_id\"]\n",
    "\n",
    "        if opt.mode == \"test\":\n",
    "            sql, norm_sql, sql_skeleton = \"\", \"\", \"\"\n",
    "            sql_tokens = []\n",
    "\n",
    "            natsql, norm_natsql, natsql_skeleton = \"\", \"\", \"\"\n",
    "            natsql_used_columns, natsql_tokens = [], []\n",
    "        else:\n",
    "\n",
    "            sql = data[\"query\"].strip()\n",
    "            norm_sql = normalization(sql).strip()\n",
    "            sql_skeleton = extract_skeleton(norm_sql, db_schemas[db_id]).strip()\n",
    "            sql_tokens = norm_sql.split()\n",
    "\n",
    "            if natsql_data is not None:\n",
    "                natsql = natsql_data[\"NatSQL\"].strip()\n",
    "                norm_natsql = normalization(natsql).strip()\n",
    "                natsql_skeleton = extract_skeleton(norm_natsql, db_schemas[db_id]).strip()\n",
    "                natsql_used_columns = [token for token in norm_natsql.split() if \".\" in token and token != \"@.@\"]\n",
    "                natsql_tokens = []\n",
    "                for token in norm_natsql.split():\n",
    "                    # split table_name_original.column_name_original\n",
    "                    if \".\" in token:\n",
    "                        natsql_tokens.extend(token.split(\".\"))\n",
    "                    else:\n",
    "                        natsql_tokens.append(token)\n",
    "            else:\n",
    "                natsql, norm_natsql, natsql_skeleton = \"\", \"\", \"\"\n",
    "                natsql_used_columns, natsql_tokens = [], []\n",
    "\n",
    "        preprocessed_data = {}\n",
    "        preprocessed_data[\"question\"] = question\n",
    "        preprocessed_data[\"db_id\"] = db_id\n",
    "\n",
    "        preprocessed_data[\"sql\"] = sql\n",
    "        preprocessed_data[\"norm_sql\"] = norm_sql\n",
    "        preprocessed_data[\"sql_skeleton\"] = sql_skeleton\n",
    "\n",
    "        preprocessed_data[\"natsql\"] = natsql\n",
    "        preprocessed_data[\"norm_natsql\"] = norm_natsql\n",
    "        preprocessed_data[\"natsql_skeleton\"] = natsql_skeleton\n",
    "\n",
    "        preprocessed_data[\"db_schema\"] = []\n",
    "        preprocessed_data[\"pk\"] = db_schemas[db_id][\"pk\"]\n",
    "        preprocessed_data[\"fk\"] = db_schemas[db_id][\"fk\"]\n",
    "        preprocessed_data[\"table_labels\"] = []\n",
    "        preprocessed_data[\"column_labels\"] = []\n",
    "\n",
    "        # add database information (including table name, column name, ..., table_labels, and column labels)\n",
    "        for table in db_schemas[db_id][\"schema_items\"]:\n",
    "            db_contents = get_db_contents(\n",
    "                question,\n",
    "                table[\"table_name_original\"],\n",
    "                table[\"column_names_original\"],\n",
    "                db_id,\n",
    "                opt.db_path\n",
    "            )\n",
    "\n",
    "            preprocessed_data[\"db_schema\"].append({\n",
    "                \"table_name_original\": table[\"table_name_original\"],\n",
    "                \"table_name\": table[\"table_name\"],\n",
    "                \"column_names\": table[\"column_names\"],\n",
    "                \"column_names_original\": table[\"column_names_original\"],\n",
    "                \"column_types\": table[\"column_types\"],\n",
    "                \"db_contents\": db_contents\n",
    "            })\n",
    "\n",
    "            # extract table and column classification labels\n",
    "            if opt.target_type == \"sql\":\n",
    "                if table[\"table_name_original\"] in sql_tokens:  # for used tables\n",
    "                    preprocessed_data[\"table_labels\"].append(1)\n",
    "                    column_labels = []\n",
    "                    for column_name_original in table[\"column_names_original\"]:\n",
    "                        if column_name_original in sql_tokens or \\\n",
    "                                table[\n",
    "                                    \"table_name_original\"] + \".\" + column_name_original in sql_tokens:  # for used columns\n",
    "                            column_labels.append(1)\n",
    "                        else:\n",
    "                            column_labels.append(0)\n",
    "                    preprocessed_data[\"column_labels\"].append(column_labels)\n",
    "                else:  # for unused tables and their columns\n",
    "                    preprocessed_data[\"table_labels\"].append(0)\n",
    "                    preprocessed_data[\"column_labels\"].append([0 for _ in range(len(table[\"column_names_original\"]))])\n",
    "            elif opt.target_type == \"natsql\":\n",
    "                if table[\"table_name_original\"] in natsql_tokens:  # for used tables\n",
    "                    preprocessed_data[\"table_labels\"].append(1)\n",
    "                    column_labels = []\n",
    "                    for column_name_original in table[\"column_names_original\"]:\n",
    "                        if table[\n",
    "                            \"table_name_original\"] + \".\" + column_name_original in natsql_used_columns:  # for used columns\n",
    "                            column_labels.append(1)\n",
    "                        else:\n",
    "                            column_labels.append(0)\n",
    "                    preprocessed_data[\"column_labels\"].append(column_labels)\n",
    "                else:\n",
    "                    preprocessed_data[\"table_labels\"].append(0)\n",
    "                    preprocessed_data[\"column_labels\"].append([0 for _ in range(len(table[\"column_names_original\"]))])\n",
    "            else:\n",
    "                raise ValueError(\"target_type should be ``sql'' or ``natsql''\")\n",
    "\n",
    "        preprocessed_dataset.append(preprocessed_data)\n",
    "\n",
    "    with open(opt.output_dataset_path, \"w\") as f:\n",
    "        preprocessed_dataset_str = json.dumps(preprocessed_dataset, indent=2)\n",
    "        f.write(preprocessed_dataset_str)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_option()\n",
    "    main(opt) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
